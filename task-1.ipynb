{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1bf587-13bb-476c-96af-046b42d35284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 10-01-2023| 50000|\n|       102|       Alice|    Finance| 15-02-2023| 70000|\n|       103|        Mark|Engineering| 20-03-2023| 85000|\n|       104|        Emma|      Sales| 01-04-2023| 55000|\n|       105|        Liam|  Marketing| 12-05-2023| 60000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import *\n",
    "spark=SparkSession.builder.appName(\"sep 17\").getOrCreate()\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/Employee.csv\",\"dbfs:/Filestore/Employee.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/Filestore/products.json\")\n",
    "employee_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/Employee.csv\")\n",
    "employee_df.show()\n",
    "products_df=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/products.json\")\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_delta\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products_delta\")\n",
    "employee_delta = spark.read.format(\"delta\").load(\"/delta/employee_delta\")\n",
    "products_delta = spark.read.format(\"delta\").load(\"/delta/products_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79802d5-5188-4e35-b032-48dc5d913e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.cp(\"File:/Workspace/Shared/new_employee.csv\",\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_new_delta\")\n",
    "employee_new_delta=spark.read.format(\"delta\").load(\"/delta/employee_new_delta\")\n",
    "employee_delta.createOrReplaceTempView(\"employee_delta\")\n",
    "employee_new_delta.createOrReplaceTempView(\"new_employee_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d1d58b-37d8-4c76-8462-486aa6143120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 10-01-2023| 50000|\n|       103|        Mark|Engineering| 20-03-2023| 85000|\n|       104|        Emma|      Sales| 01-04-2023| 55000|\n|       105|        Liam|  Marketing| 12-05-2023| 60000|\n|       102|       Alice|    Finance| 15-02-2023| 75000|\n|       106|      Olivia|       NULL| 10-06-2023| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "          merge into employee_delta as target\n",
    "          using new_employee_delta as source\n",
    "          on target.employeeID=source.employeeID\n",
    "          when matched then update set target.Salary=source.Salary\n",
    "          when not matched then \n",
    "           insert (EmployeeID, EmployeeName, JoiningDate, Salary)\n",
    "           values (source.EmployeeID, source.EmployeeName, source.JoiningDate, source.Salary)\n",
    "          \"\"\")\n",
    "spark.sql(\"select * from employee_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767120b3-bcad-4d6f-9089-27642adcdbab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|           operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      2|2024-09-17 05:32:13|3885359369260198|azuser2114_mml.lo...|          VACUUM END|{status -> COMPLE...|NULL|{2164644895540858}|0913-035629-qk1t69gv|          1|SnapshotIsolation|         true|{numDeletedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2024-09-17 05:32:07|3885359369260198|azuser2114_mml.lo...|        VACUUM START|{retentionCheckEn...|NULL|{2164644895540858}|0913-035629-qk1t69gv|          0|SnapshotIsolation|         true|{numFilesToDelete...|        NULL|Databricks-Runtim...|\n|      0|2024-09-17 05:26:20|3885359369260198|azuser2114_mml.lo...|CREATE TABLE AS S...|{partitionBy -> [...|NULL|{2164644895540858}|0913-035629-qk1t69gv|       NULL|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[EmployeeID: string, EmployeeName: string, Department: string, JoiningDate: string, Salary: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          create table if not exists employee  as select * from employee_delta\n",
    "          \"\"\")\n",
    "#Optmizing the table using zordering and optimize\n",
    "spark.sql(\"optimize employee zorder by(Salary)\")\n",
    "\n",
    "#describing the history of the delta table\n",
    "spark.sql(\"DESCRIBE HISTORY employee\").show()\n",
    "\n",
    "#vacuuming the table abd storing data of previous 7 days only\n",
    "spark.sql(\"Vacuum employee retain 168 hours\")\n",
    "\n",
    "#using versioning of delta lake to find data with certain version\n",
    "spark.sql(\"SELECT * FROM employee VERSION AS OF 3\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2164644895540875,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "10:13:02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
