{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Streaming and Transformations on Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/transactions.csv\", \"dbfs:/FileStore/streaming/input/transactions.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Ingest Streaming Data from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/streaming/input/transactions.csv\")\n",
    "schema = static_df.schema\n",
    "\n",
    "streaming_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"dbfs:/FileStore/streaming/input/\")\n",
    "\n",
    "query = streaming_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Stream Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column for the TotalAmount ( Quantity * Price ).\n",
    "#Filter records where the Quantity is greater than 1.\n",
    "transformed_df = streaming_df.withColumn(\"TotalAmount\", streaming_df[\"Quantity\"] * streaming_df[\"Price\"]).filter(streaming_df[\"Quantity\"] > 1)\n",
    "\n",
    "query = transformed_df.writeStream.format(\"memory\").queryName(\"transformed_stream\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Aggregations on Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "#Group the data by ProductID and calculate the total sales for each product\n",
    "aggregated_df = streaming_df.groupBy(\"ProductID\").agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalSales\"))\n",
    "query = aggregated_df.writeStream.format(\"console\").outputMode(\"update\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Writing Streaming Data to File Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = transformed_df.writeStream.format(\"parquet\").option(\"path\", \"/dbfs/FileStore/parquet\") \\\n",
    "                                   .option(\"checkpointLocation\", \"/dbfs/FileStore/checkpoint\") \\\n",
    "                                   .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Handling Late Data using Watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"TransactionDate\", to_timestamp(col(\"TransactionDate\")))\n",
    "\n",
    "watermarked_df = streaming_df.withWatermark(\"TransactionDate\", \"1 day\")\n",
    "\n",
    "watermarked_query = watermarked_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Streaming from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1: Incoming transaction data (CSV)\n",
    "transactions_stream = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"basePath\", \"dbfs:/FileStore/streaming/input/\") \\\n",
    "    .schema(\"TransactionID STRING, TransactionDate DATE, ProductID STRING, Quantity INT, Price DOUBLE\") \\\n",
    "    .load(\"dbfs:/FileStore/streaming/input/\")\n",
    "\n",
    "# Stream 2: Product information (JSON)\n",
    "products_stream = spark.readStream.format(\"json\") \\\n",
    "    .option(\"basePath\", \"dbfs:/FileStore/streaming/input/\") \\\n",
    "    .schema(\"ProductID STRING, ProductName STRING, Category STRING\") \\\n",
    "    .load(\"dbfs:/FileStore/streaming/input/\")\n",
    "\n",
    "# Join both streams on ProductID\n",
    "joined_stream = transactions_stream.join(products_stream, \"ProductID\")\n",
    "\n",
    "# Write the joined stream to the console to visualize results\n",
    "query = joined_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
