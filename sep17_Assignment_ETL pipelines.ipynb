{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Complete ETL Pipeline using Delta Live Tables (DLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/orders.csv\", \"dbfs:/FileStore/streaming/input/orders.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Create an ETL Pipeline using DLT (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>incremental_orders</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalAmount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=419276616380425#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2141_mml.local%40techademy.com%2FSep%2017&redirectNotebookId=418182220691607\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "@dlt.table\n",
    "def transformed_orders():\n",
    "    # Read CSV source\n",
    "    df = spark.read.csv(\"dbfs:/FileStore/streaming/input/orders.csv\", header=True)\n",
    "    \n",
    "    # Transform data\n",
    "    df_transformed = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "                       .filter(col(\"Quantity\") > 1)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "@dlt.table\n",
    "def incremental_orders():\n",
    "    return dlt.read_stream(\"transformed_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Create an ETL Pipeline using DLT (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_orders = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/streaming/input/orders.csv\")\n",
    "\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>transformed_orders</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>date</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalAmount</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=419276616380425#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2141_mml.local%40techademy.com%2FSep%2017&redirectNotebookId=418182220691607\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE LIVE TABLE transformed_orders AS\n",
    "SELECT *, Quantity * Price AS TotalAmount\n",
    "FROM delta.`/delta/orders`\n",
    "WHERE Quantity > 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Perform Read, Write, Update, and Delete Operations on Delta Table (SQL + PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the Data from the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "|    101|2024-01-01|      C001| Laptop|       2| 1000|\n",
      "|    102|2024-01-02|      C002|  Phone|       1|  500|\n",
      "|    103|2024-01-03|      C003| Tablet|       3|  300|\n",
      "|    104|2024-01-04|      C004|Monitor|       1|  150|\n",
      "|    105|2024-01-05|      C005|  Mouse|       5|   20|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the Delta table in PySpark\n",
    "df = spark.read.format(\"delta\").load(\"/delta/orders\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderID</th><th>OrderDate</th><th>CustomerID</th><th>Product</th><th>Quantity</th><th>Price</th></tr></thead><tbody><tr><td>101</td><td>2024-01-01</td><td>C001</td><td>Laptop</td><td>2</td><td>1000</td></tr><tr><td>102</td><td>2024-01-02</td><td>C002</td><td>Phone</td><td>1</td><td>500</td></tr><tr><td>103</td><td>2024-01-03</td><td>C003</td><td>Tablet</td><td>3</td><td>300</td></tr><tr><td>104</td><td>2024-01-04</td><td>C004</td><td>Monitor</td><td>1</td><td>150</td></tr><tr><td>105</td><td>2024-01-05</td><td>C005</td><td>Mouse</td><td>5</td><td>20</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Reading the Delta table using SQL\n",
    "SELECT * FROM delta.`/delta/orders`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Update the Price of a Product (e.g., increase Laptop price by 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/delta/orders\")\n",
    "\n",
    "# Update the Price of 'Laptop' by increasing it by 10%\n",
    "delta_table.update(\n",
    "    condition = expr(\"Product = 'Laptop'\"),\n",
    "    set = { \"Price\": expr(\"Price * 1.1\") }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Update product price using SQL (increase Laptop price by 10%)\n",
    "UPDATE delta.`/delta/orders`\n",
    "SET Price = Price * 1.1\n",
    "WHERE Product = 'Laptop';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Delete Rows where Quantity is Less Than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where Quantity is less than 2\n",
    "delta_table.delete(condition = expr(\"Quantity < 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Delete rows where Quantity is less than 2\n",
    "DELETE FROM delta.`/delta/orders`\n",
    "WHERE Quantity < 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Insert a New Record into the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "# Insert a new record into the Delta table\n",
    "new_data = [(106, 'C006', 'Keyboard', 2, 50)]\n",
    "columns = [\"OrderID\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "\n",
    "# Define the schema of the Delta table\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), nullable=False),\n",
    "    StructField(\"CustomerID\", StringType(), nullable=False),\n",
    "    StructField(\"Product\", StringType(), nullable=False),\n",
    "    StructField(\"Quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"Price\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create a DataFrame with new data and specified schema\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "# Append the new data to the Delta table\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(\"/delta/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Insert a new record using SQL\n",
    "INSERT INTO delta.`/delta/orders`\n",
    "VALUES (107, '2024-01-06', 'C006', 'Keyboard', 2, 50);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Merge Data (Slowly Changing Dimension - SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data representing updated orders\n",
    "new_data = [(101, '2024-01-10', 'C001', 'Laptop', 2, 1200),  # Updated order for Laptop\n",
    "            (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)]   # New order for Keyboard\n",
    "\n",
    "columns = [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "\n",
    "# Create a DataFrame with the new data\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Merge the new data into the Delta table (SCD Type 2)\n",
    "delta_table.alias(\"old\").merge(\n",
    "    new_df.alias(\"new\"),\n",
    "    \"old.OrderID = new.OrderID\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"OrderDate\": \"new.OrderDate\",\n",
    "    \"CustomerID\": \"new.CustomerID\",\n",
    "    \"Product\": \"new.Product\",\n",
    "    \"Quantity\": \"new.Quantity\",\n",
    "    \"Price\": \"new.Price\"\n",
    "}).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Explore Delta Table Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|          userId|            userName|operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     35|2024-09-17 05:04:39|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         34|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     34|2024-09-17 05:04:37|2929584751483774|azuser2141_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         33|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n",
      "|     33|2024-09-17 04:58:42|2929584751483774|azuser2141_mml.lo...|    WRITE|{mode -> Append, ...|NULL|{418182220691607}|0912-053226-p5usobai|         32|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|     32|2024-09-17 04:57:46|2929584751483774|azuser2141_mml.lo...|    WRITE|{mode -> Append, ...|NULL|{418182220691607}|0912-053226-p5usobai|         31|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|     31|2024-09-17 04:56:02|2929584751483774|azuser2141_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         30|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     30|2024-09-17 04:55:27|2929584751483774|azuser2141_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         29|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     29|2024-09-17 04:55:01|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         28|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     28|2024-09-17 04:54:58|2929584751483774|azuser2141_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         27|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     27|2024-09-17 04:54:36|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         26|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     26|2024-09-17 04:54:34|2929584751483774|azuser2141_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         25|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     25|2024-09-17 04:53:11|2929584751483774|azuser2141_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{418182220691607}|0912-053226-p5usobai|         24|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|     24|2024-09-17 04:51:47|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         23|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     23|2024-09-17 04:51:46|2929584751483774|azuser2141_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         22|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     22|2024-09-17 04:51:44|2929584751483774|azuser2141_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         21|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     21|2024-09-17 04:50:56|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         20|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     20|2024-09-17 04:50:54|2929584751483774|azuser2141_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         19|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     19|2024-09-17 04:50:53|2929584751483774|azuser2141_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         18|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     18|2024-09-17 04:50:07|2929584751483774|azuser2141_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{418182220691607}|0912-053226-p5usobai|         17|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     17|2024-09-17 04:50:05|2929584751483774|azuser2141_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         16|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|     16|2024-09-17 04:50:04|2929584751483774|azuser2141_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{418182220691607}|0912-053226-p5usobai|         15|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+--------------------+----+-----------+------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n",
      "|format|                  id|name|description|          location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|          properties|minReaderVersion|minWriterVersion|    tableFeatures|          statistics|\n",
      "+------+--------------------+----+-----------+------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n",
      "| delta|505a9da5-3e4f-469...|NULL|       NULL|dbfs:/delta/orders|2024-09-17 04:41:...|2024-09-17 05:04:39|              []|               []|       1|       1594|{delta.enableDele...|               3|               7|[deletionVectors]|{numRowsDeletedBy...|\n",
      "+------+--------------------+----+-----------+------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the transaction history using PySpark\n",
    "delta_table.history().show()\n",
    "# Check file details using PySpark\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`/delta/orders`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>505a9da5-3e4f-469c-ba9c-1974afaed544</td><td>null</td><td>null</td><td>dbfs:/delta/orders</td><td>2024-09-17T04:41:18.952Z</td><td>2024-09-17T05:04:39Z</td><td>List()</td><td>List()</td><td>1</td><td>1594</td><td>Map(delta.enableDeletionVectors -> true)</td><td>3</td><td>7</td><td>List(deletionVectors)</td><td>Map(numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Check the transaction history using SQL\n",
    "DESCRIBE HISTORY delta.`/delta/orders`;\n",
    "-- Check file details using SQL\n",
    "DESCRIBE DETAIL delta.`/delta/orders`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Time Travel in Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "|    102|2024-01-02|      C002|  Phone|       1|  500|\n",
      "|    103|2024-01-03|      C003| Tablet|       3|  300|\n",
      "|    104|2024-01-04|      C004|Monitor|       1|  150|\n",
      "|    105|2024-01-05|      C005|  Mouse|       5|   20|\n",
      "|    101|2024-01-01|      C001| Laptop|       2| 1100|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "|    101|2024-01-01|      C001| Laptop|       2| 1000|\n",
      "|    102|2024-01-02|      C002|  Phone|       1|  500|\n",
      "|    103|2024-01-03|      C003| Tablet|       3|  300|\n",
      "|    104|2024-01-04|      C004|Monitor|       1|  150|\n",
      "|    105|2024-01-05|      C005|  Mouse|       5|   20|\n",
      "+-------+----------+----------+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table as it existed at a specific version\n",
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/orders\")\n",
    "df_version.show()\n",
    "\n",
    "# Query the table as it existed at a specific timestamp\n",
    "df_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-09-17T04:41:20Z\").load(\"/delta/orders\")\n",
    "df_timestamp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Optimize Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimize the table and Z-order by Product column\n",
    "spark.sql(\"OPTIMIZE delta.`/delta/orders` ZORDER BY (Product)\")\n",
    "# Vacuum the Delta table to remove old versions of the files\n",
    "spark.sql(\"VACUUM delta.`/delta/orders` RETAIN 168 HOURS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Converting Parquet Files to Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-------+--------+-----+\n",
      "|OrderID|OrderDate|CustomerID|Product|Quantity|Price|\n",
      "+-------+---------+----------+-------+--------+-----+\n",
      "+-------+---------+----------+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Define the schema of your Parquet file\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "parquet_path = \"/dbfs/FileStore/\"\n",
    "delta_path = \"/dbfs/FileStore/delta_table\"\n",
    "\n",
    "# Read Parquet data with the defined schema\n",
    "df_parquet = spark.read.format(\"parquet\").schema(schema).load(parquet_path)\n",
    "\n",
    "# Write the data in Delta format\n",
    "df_parquet.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Register the Delta table\n",
    "spark.sql(f\"CREATE TABLE delta_table USING DELTA LOCATION '{delta_path}'\")\n",
    "\n",
    "# Query the newly converted Delta table\n",
    "df_converted = spark.sql(\"SELECT * FROM delta_table\")\n",
    "df_converted.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
