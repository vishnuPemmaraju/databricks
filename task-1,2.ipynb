{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1bf587-13bb-476c-96af-046b42d35284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import *\n",
    "spark=SparkSession.builder.appName(\"sep 17\").getOrCreate()\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/Employee.csv\",\"dbfs:/Filestore/Employee.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/Filestore/products.json\")\n",
    "employee_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/Employee.csv\")\n",
    "employee_df.show()\n",
    "products_df=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/products.json\")\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_delta\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products_delta\")\n",
    "employee_delta = spark.read.format(\"delta\").load(\"/delta/employee_delta\")\n",
    "products_delta = spark.read.format(\"delta\").load(\"/delta/products_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79802d5-5188-4e35-b032-48dc5d913e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.cp(\"File:/Workspace/Shared/new_employee.csv\",\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_new_delta\")\n",
    "employee_new_delta=spark.read.format(\"delta\").load(\"/delta/employee_new_delta\")\n",
    "employee_delta.createOrReplaceTempView(\"employee_delta\")\n",
    "employee_new_delta.createOrReplaceTempView(\"new_employee_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d1d58b-37d8-4c76-8462-486aa6143120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "          merge into employee_delta as target\n",
    "          using new_employee_delta as source\n",
    "          on target.employeeID=source.employeeID\n",
    "          when matched then update set target.Salary=source.Salary\n",
    "          when not matched then \n",
    "           insert (EmployeeID, EmployeeName, JoiningDate, Salary)\n",
    "           values (source.EmployeeID, source.EmployeeName, source.JoiningDate, source.Salary)\n",
    "          \"\"\")\n",
    "spark.sql(\"select * from employee_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767120b3-bcad-4d6f-9089-27642adcdbab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          create table if not exists employee  as select * from employee_delta\n",
    "          \"\"\")\n",
    "#Optmizing the table using zordering and optimize\n",
    "spark.sql(\"optimize employee zorder by(Salary)\")\n",
    "\n",
    "#describing the history of the delta table\n",
    "spark.sql(\"DESCRIBE HISTORY employee\").show()\n",
    "\n",
    "#vacuuming the table abd storing data of previous 7 days only\n",
    "spark.sql(\"Vacuum employee retain 168 hours\")\n",
    "\n",
    "#using versioning of delta lake to find data with certain version\n",
    "spark.sql(\"SELECT * FROM employee VERSION AS OF 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60840632-8f0f-4db3-a26d-c05b57bb9a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/transaction.csv\",\"dbfs:/streaming/input/transaction.csv\")\n",
    "transaction_schema=\"transactionID String, transactionDate DATE, productID STRING,Quantity INT,Price INT\"\n",
    "transaction_stream=spark.readStream.format(\"csv\").option(\"header\",\"true\").schema(transaction_schema).load(\"dbfs:/streaming/input/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2aa9ec-9b8d-4e66-a85a-6d7c732a3136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sales data by product...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,current_date, datediff, to_timestamp\n",
    "transaction_transformed=transaction_stream.select(\n",
    "    col(\"transactionID\"),\n",
    "    to_timestamp(col(\"TransactionDate\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"transactionDate\"),\n",
    "    col(\"productID\"),\n",
    "    col(\"Quantity\"),\n",
    "    col(\"Price\"),\n",
    "    (col(\"Quantity\")*col(\"price\")).alias(\"Total Price\")\n",
    ")\n",
    "transaction_filter=transaction_transformed.filter((col(\"Quantity\")>1).alias(\"Quantity\"))\n",
    "\n",
    "transactions_aggregated = transaction_transformed \\\n",
    ".withWatermark(\"transactionDate\", \"1 day\") \\\n",
    ".groupBy(\"ProductID\") \\\n",
    ".agg({\"Total Price\": \"sum\"})\n",
    "print(\"Aggregated sales data by product...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631b7694-7f77-4c1e-85ee-2e3776cd76db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fb978a90690>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the checkpoint and output locations\n",
    "checkpoint_location = \"/delta/checkpoints/transaction_parquet_checkpoint\"\n",
    "output_path = \"/delta/output/transaction_parquet_output\"\n",
    "\n",
    "# Write the streaming DataFrame to a Parquet sink\n",
    "transaction_transformed.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c6d156-bcc9-447d-a19f-b07410e74575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/Filestore/products.json\")\n",
    "product_schema=\"productId String,productName String,category String,Price INT\"\n",
    "product_stream=spark.readStream.format(\"json\").option(\"multiline\",\"true\").schema(product_schema).load(\"dbfs:/Filestore/products.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ef2524-a7ca-426a-ad72-f605e26be2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform the join on the two streaming DataFrames\n",
    "output_path = \"/delta/output/combined_stream_output\"  # Ensure this is a directory\n",
    "checkpoint_location = \"/delta/checkpoints/combined_stream_checkpoint\"  # Ensure this is a directory\n",
    "\n",
    "combined_stream = transaction_transformed \\\n",
    "    .join(product_stream, on=\"ProductID\", how=\"inner\")  # Use \"inner\" join; adjust as needed\n",
    "# Write the joined stream to the console\n",
    "query = combined_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"path\", output_path)\\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2164644895540875,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "task-1,2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
