{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1bf587-13bb-476c-96af-046b42d35284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import *\n",
    "spark=SparkSession.builder.appName(\"sep 17\").getOrCreate()\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/Employee.csv\",\"dbfs:/Filestore/Employee.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/Filestore/products.json\")\n",
    "employee_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/Employee.csv\")\n",
    "employee_df.show()\n",
    "products_df=spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"/content/products.json\")\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_delta\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products_delta\")\n",
    "employee_delta = spark.read.format(\"delta\").load(\"/delta/employee_delta\")\n",
    "products_delta = spark.read.format(\"delta\").load(\"/delta/products_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79802d5-5188-4e35-b032-48dc5d913e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.cp(\"File:/Workspace/Shared/new_employee.csv\",\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/new_employee.csv\")\n",
    "employee_new_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_new_delta\")\n",
    "employee_new_delta=spark.read.format(\"delta\").load(\"/delta/employee_new_delta\")\n",
    "employee_delta.createOrReplaceTempView(\"employee_delta\")\n",
    "employee_new_delta.createOrReplaceTempView(\"new_employee_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d1d58b-37d8-4c76-8462-486aa6143120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "          merge into employee_delta as target\n",
    "          using new_employee_delta as source\n",
    "          on target.employeeID=source.employeeID\n",
    "          when matched then update set target.Salary=source.Salary\n",
    "          when not matched then \n",
    "           insert (EmployeeID, EmployeeName, JoiningDate, Salary)\n",
    "           values (source.EmployeeID, source.EmployeeName, source.JoiningDate, source.Salary)\n",
    "          \"\"\")\n",
    "spark.sql(\"select * from employee_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767120b3-bcad-4d6f-9089-27642adcdbab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          create table if not exists employee  as select * from employee_delta\n",
    "          \"\"\")\n",
    "#Optmizing the table using zordering and optimize\n",
    "spark.sql(\"optimize employee zorder by(Salary)\")\n",
    "\n",
    "#describing the history of the delta table\n",
    "spark.sql(\"DESCRIBE HISTORY employee\").show()\n",
    "\n",
    "#vacuuming the table abd storing data of previous 7 days only\n",
    "spark.sql(\"Vacuum employee retain 168 hours\")\n",
    "\n",
    "#using versioning of delta lake to find data with certain version\n",
    "spark.sql(\"SELECT * FROM employee VERSION AS OF 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60840632-8f0f-4db3-a26d-c05b57bb9a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/transaction.csv\",\"dbfs:/Filestore/streaming/input/transaction.csv\")\n",
    "transaction_schema=\"transactionID String, transactionDate DATE, productID STRING,Quantity INT,Price INT\"\n",
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/Filestore/streaming/input/transaction.csv\")\n",
    "schema = static_df.schema\n",
    "streaming_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"dbfs:/Filestore/streaming/input/\")\n",
    "query = streaming_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1b1d60-94be-44c9-8d67-1e1bced23559",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = streaming_df.withColumn(\"TotalAmount\", streaming_df[\"Quantity\"] * streaming_df[\"Price\"]).filter(streaming_df[\"Quantity\"] > 1)\n",
    "query = transformed_df.writeStream.format(\"memory\").queryName(\"transformed_stream\").start()\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "#Group the data by ProductID and calculate the total sales for each product\n",
    "aggregated_df = streaming_df.groupBy(\"ProductID\").agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalSales\"))\n",
    "query = aggregated_df.writeStream.format(\"console\").outputMode(\"update\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f5d589-d195-48e2-a97c-5ab9aa844687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = transformed_df.writeStream.format(\"parquet\").option(\"path\", \"/dbfs/FileStore/parquet\") \\\n",
    "                                   .option(\"checkpointLocation\", \"/dbfs/FileStore/checkpoint\") \\\n",
    "                                   .start()\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"TransactionDate\", to_timestamp(col(\"TransactionDate\")))\n",
    "watermarked_df = streaming_df.withWatermark(\"TransactionDate\", \"1 day\")\n",
    "watermarked_query = watermarked_df.writeStream.format(\"console\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fcd2d3-2e96-415a-a72e-30a23d31cc3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stream 1: Incoming transaction data (CSV)\n",
    "transactions_stream = spark.readStream.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"basePath\", \"dbfs:/Filestore/streaming/input/\") \\\n",
    "    .schema(\"TransactionID STRING, TransactionDate DATE, ProductID STRING, Quantity INT, Price DOUBLE\") \\\n",
    "    .load(\"dbfs:/Filestore/streaming/input/\")\n",
    "\n",
    "# Stream 2: Product information (JSON)\n",
    "products_stream = spark.readStream.format(\"json\") \\\n",
    "    .option(\"basePath\", \"dbfs:/Filestore/streaming/input/\") \\\n",
    "    .schema(\"ProductID STRING, ProductName STRING, Category STRING\") \\\n",
    "    .load(\"dbfs:/Filestore/streaming/input/\")\n",
    "\n",
    "# Join both streams on ProductID\n",
    "joined_stream = transactions_stream.join(products_stream, \"ProductID\")\n",
    "\n",
    "# Write the joined stream to the console to visualize results\n",
    "query = joined_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583ca91-a4c8-4323-9628-64d37e3f2311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/orders.csv\",\"dbfs:/Filestore/streaming/input/orders.csv\")\n",
    "orders_schema=\"OrderID String,OrderDate Date,CustomerID String,Product String,Quantity INT,Price INT\"\n",
    "orders_stream=spark.readStream.format(\"csv\").option(\"header\",\"true\").schema(orders_schema).load(\"dbfs:/Filestore/streaming/input/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582c3108-48ab-4539-872e-f67763ae0044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting DLT\n  Obtaining dependency information for DLT from https://files.pythonhosted.org/packages/8c/42/522adf6823e4b1e9ea4930dc01695e958662b6bb0141d7be47c5f2826441/dlt-1.0.0-py3-none-any.whl.metadata\n  Downloading dlt-1.0.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: PyYAML>=5.4.1 in /databricks/python3/lib/python3.11/site-packages (from DLT) (6.0)\nRequirement already satisfied: astunparse>=1.6.3 in /databricks/python3/lib/python3.11/site-packages (from DLT) (1.6.3)\nRequirement already satisfied: click>=7.1 in /databricks/python3/lib/python3.11/site-packages (from DLT) (8.0.4)\nCollecting fsspec>=2022.4.0 (from DLT)\n  Obtaining dependency information for fsspec>=2022.4.0 from https://files.pythonhosted.org/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl.metadata\n  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: gitpython>=3.1.29 in /databricks/python3/lib/python3.11/site-packages (from DLT) (3.1.43)\nCollecting giturlparse>=0.10.0 (from DLT)\n  Obtaining dependency information for giturlparse>=0.10.0 from https://files.pythonhosted.org/packages/dd/94/c6ff3388b8e3225a014e55aed957188639aa0966443e0408d38f0c9614a7/giturlparse-0.12.0-py2.py3-none-any.whl.metadata\n  Downloading giturlparse-0.12.0-py2.py3-none-any.whl.metadata (4.5 kB)\nCollecting hexbytes>=0.2.2 (from DLT)\n  Obtaining dependency information for hexbytes>=0.2.2 from https://files.pythonhosted.org/packages/39/c6/20f25ea73e4ceffb3eb4e38347f2992cb25e5ff6eb644d52e753a7a72f57/hexbytes-1.2.1-py3-none-any.whl.metadata\n  Downloading hexbytes-1.2.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting humanize>=4.4.0 (from DLT)\n  Obtaining dependency information for humanize>=4.4.0 from https://files.pythonhosted.org/packages/8f/49/a29c79bea335e52fb512a43faf84998c184c87fef82c65f568f8c56f2642/humanize-4.10.0-py3-none-any.whl.metadata\n  Downloading humanize-4.10.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting jsonpath-ng>=1.5.3 (from DLT)\n  Obtaining dependency information for jsonpath-ng>=1.5.3 from https://files.pythonhosted.org/packages/16/0a/3b1ee3721b4bd684b0e29c724ab82ed3b2c0e42285beb8bf9e18de8c903f/jsonpath_ng-1.6.1-py3-none-any.whl.metadata\n  Downloading jsonpath_ng-1.6.1-py3-none-any.whl.metadata (18 kB)\nCollecting makefun>=1.15.0 (from DLT)\n  Obtaining dependency information for makefun>=1.15.0 from https://files.pythonhosted.org/packages/7a/9d/655cbedbe6df11126bae5752ca41bd8cf5180a01c4148577fc547a4c0b14/makefun-1.15.4-py2.py3-none-any.whl.metadata\n  Downloading makefun-1.15.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson!=3.10.1,!=3.9.11,!=3.9.12,!=3.9.13,!=3.9.14,<4,>=3.6.7 (from DLT)\n  Obtaining dependency information for orjson!=3.10.1,!=3.9.11,!=3.9.12,!=3.9.13,!=3.9.14,<4,>=3.6.7 from https://files.pythonhosted.org/packages/d3/cb/55205f3f1ee6ba80c0a9a18ca07423003ca8de99192b18be30f1f31b4cdd/orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/50.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/50.4 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.4/50.4 kB\u001B[0m \u001B[31m767.7 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: packaging>=21.1 in /databricks/python3/lib/python3.11/site-packages (from DLT) (23.2)\nCollecting pathvalidate>=2.5.2 (from DLT)\n  Obtaining dependency information for pathvalidate>=2.5.2 from https://files.pythonhosted.org/packages/d3/5e/76a9d08b4b4e4583f269cb9f64de267f9aeae0dacef23307f53a14211716/pathvalidate-3.2.1-py3-none-any.whl.metadata\n  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\nCollecting pendulum>=2.1.2 (from DLT)\n  Obtaining dependency information for pendulum>=2.1.2 from https://files.pythonhosted.org/packages/84/3a/5e36479e199a034adcf6a1a95c691f0a2781ea55b9ac3bcb887e2f97d82b/pendulum-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pendulum-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: pytz>=2022.6 in /databricks/python3/lib/python3.11/site-packages (from DLT) (2022.7)\nRequirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.11/site-packages (from DLT) (2.31.0)\nCollecting requirements-parser>=0.5.0 (from DLT)\n  Obtaining dependency information for requirements-parser>=0.5.0 from https://files.pythonhosted.org/packages/88/33/190393a7d36872e237cbc99e6c44d9a078a1ba7b406462fe6eafd5a28e04/requirements_parser-0.11.0-py3-none-any.whl.metadata\n  Downloading requirements_parser-0.11.0-py3-none-any.whl.metadata (4.7 kB)\nCollecting semver>=2.13.0 (from DLT)\n  Obtaining dependency information for semver>=2.13.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: setuptools>=65.6.0 in /databricks/python3/lib/python3.11/site-packages (from DLT) (68.0.0)\nCollecting simplejson>=3.17.5 (from DLT)\n  Obtaining dependency information for simplejson>=3.17.5 from https://files.pythonhosted.org/packages/b7/d4/850948bcbcfe0b4a6c69dfde10e245d3a1ea45252f16a1e2308a3b06b1da/simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: tenacity>=8.0.2 in /databricks/python3/lib/python3.11/site-packages (from DLT) (8.2.2)\nCollecting tomlkit>=0.11.3 (from DLT)\n  Obtaining dependency information for tomlkit>=0.11.3 from https://files.pythonhosted.org/packages/f9/b6/a447b5e4ec71e13871be01ba81f5dfc9d0af7e473da256ff46bc0e24026f/tomlkit-0.13.2-py3-none-any.whl.metadata\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from DLT) (4.10.0)\nRequirement already satisfied: tzdata>=2022.1 in /databricks/python3/lib/python3.11/site-packages (from DLT) (2022.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from astunparse>=1.6.3->DLT) (0.38.4)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/lib/python3/dist-packages (from astunparse>=1.6.3->DLT) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython>=3.1.29->DLT) (4.0.11)\nCollecting ply (from jsonpath-ng>=1.5.3->DLT)\n  Obtaining dependency information for ply from https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl.metadata\n  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\nRequirement already satisfied: python-dateutil>=2.6 in /databricks/python3/lib/python3.11/site-packages (from pendulum>=2.1.2->DLT) (2.8.2)\nCollecting time-machine>=2.6.0 (from pendulum>=2.1.2->DLT)\n  Obtaining dependency information for time-machine>=2.6.0 from https://files.pythonhosted.org/packages/13/87/a6de1b187f5468781e0e722c877323625227151cc8ffff363a7391b01149/time_machine-2.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading time_machine-2.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.26.0->DLT) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.26.0->DLT) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.26.0->DLT) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.26.0->DLT) (2023.7.22)\nCollecting types-setuptools>=69.1.0 (from requirements-parser>=0.5.0->DLT)\n  Obtaining dependency information for types-setuptools>=69.1.0 from https://files.pythonhosted.org/packages/40/4c/a4c87d86ba18ff00773ab8591c79c23a6938293ab3e2cec2b2eb4ca5b644/types_setuptools-75.1.0.20240917-py3-none-any.whl.metadata\n  Downloading types_setuptools-75.1.0.20240917-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.29->DLT) (5.0.1)\nDownloading dlt-1.0.0-py3-none-any.whl (803 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/804.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.9/804.0 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m174.1/804.0 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m286.7/804.0 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m634.9/804.0 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m804.0/804.0 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/179.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading giturlparse-0.12.0-py2.py3-none-any.whl (15 kB)\nDownloading hexbytes-1.2.1-py3-none-any.whl (5.2 kB)\nDownloading humanize-4.10.0-py3-none-any.whl (126 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/127.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.0/127.0 kB\u001B[0m \u001B[31m22.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\nDownloading makefun-1.15.4-py2.py3-none-any.whl (23 kB)\nDownloading orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/141.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m141.9/141.9 kB\u001B[0m \u001B[31m20.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\nDownloading pendulum-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/384.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m384.9/384.9 kB\u001B[0m \u001B[31m16.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requirements_parser-0.11.0-py3-none-any.whl (14 kB)\nDownloading semver-3.0.2-py3-none-any.whl (17 kB)\nDownloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/144.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m144.7/144.7 kB\u001B[0m \u001B[31m22.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading time_machine-2.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\nDownloading types_setuptools-75.1.0.20240917-py3-none-any.whl (65 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/65.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.5/65.5 kB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/49.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.6/49.6 kB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: ply, makefun, types-setuptools, tomlkit, simplejson, semver, pathvalidate, orjson, jsonpath-ng, humanize, hexbytes, giturlparse, fsspec, time-machine, requirements-parser, pendulum, DLT\nSuccessfully installed DLT-1.0.0 fsspec-2024.9.0 giturlparse-0.12.0 hexbytes-1.2.1 humanize-4.10.0 jsonpath-ng-1.6.1 makefun-1.15.4 orjson-3.10.7 pathvalidate-3.2.1 pendulum-3.0.0 ply-3.11 requirements-parser-0.11.0 semver-3.0.2 simplejson-3.19.3 time-machine-2.15.0 tomlkit-0.13.2 types-setuptools-75.1.0.20240917\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ffc4ff-ff24-4887-9b62-fdfd91dae547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>orders_streamed</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalPrice</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=150066163839714#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2114_mml.local%40techademy.com%2Ftask-1%2C2&redirectNotebookId=1808337479142296\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def orders_transformed():\n",
    "    df_orders=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/Filestore/streaming/input/orders.csv\")\n",
    "    df_order=df_orders.withColumn(\"TotalPrice\",col(\"Quantity\")*col(\"Price\")).filter(col(\"Quantity\")>1)\n",
    "    return df_order\n",
    "\n",
    "@dlt.table\n",
    "def orders_streamed():\n",
    "    return dlt.read_stream(\"orders_transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15097a60-10a1-412c-9806-5da345364b91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/Filestore/streaming/input/orders.csv\")\n",
    "\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/orders\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f46174-4e63-48c8-9f6b-83a651b17640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE LIVE TABLE transformed_orders AS\n",
    "SELECT *, Quantity * Price AS TotalAmount\n",
    "FROM delta.`/delta/orders`\n",
    "WHERE Quantity > 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8358b929-62a5-4243-ae4c-f947edf3dc83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the Delta table in PySpark\n",
    "df = spark.read.format(\"delta\").load(\"/delta/orders\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db78c91-0d35-4a0f-966c-6b9d687b892c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Reading the Delta table using SQL\n",
    "SELECT * FROM delta.`/delta/orders`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8e94bd-d40f-4c4f-8a08-2c167d78bc86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/delta/orders\")\n",
    "\n",
    "# Update the Price of 'Laptop' by increasing it by 10%\n",
    "delta_table.update(\n",
    "    condition = expr(\"Product = 'Laptop'\"),\n",
    "    set = { \"Price\": expr(\"Price * 1.1\") }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e1e761-60c3-432c-9fd9-85706e4afefe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l\n",
    "-- Update product price using SQL (increase Laptop price by 10%)\n",
    "UPDATE delta.`/delta/orders`\n",
    "SET Price = Price * 1.1\n",
    "WHERE Product = 'Laptop';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b10e70a-dfa5-40dc-b85c-8439e864c8bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "# Insert a new record into the Delta table\n",
    "new_data = [(106, 'C006', 'Keyboard', 2, 50)]\n",
    "columns = [\"OrderID\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "\n",
    "# Define the schema of the Delta table\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), nullable=False),\n",
    "    StructField(\"CustomerID\", StringType(), nullable=False),\n",
    "    StructField(\"Product\", StringType(), nullable=False),\n",
    "    StructField(\"Quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"Price\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create a DataFrame with new data and specified schema\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "# Append the new data to the Delta table\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(\"/delta/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab6d91f5-1ef7-4d03-a47e-83dcdec8214b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/orders\")\n",
    "df_version.show()\n",
    "\n",
    "# Query the table as it existed at a specific timestamp\n",
    "df_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-09-17T04:41:20Z\").load(\"/delta/orders\")\n",
    "df_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a989f9-056d-4f0a-ab5f-e9df3e41492a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Define the schema of your Parquet file\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "parquet_path = \"/dbfs/FileStore/\"\n",
    "delta_path = \"/dbfs/FileStore/delta_table\"\n",
    "\n",
    "# Read Parquet data with the defined schema\n",
    "df_parquet = spark.read.format(\"parquet\").schema(schema).load(parquet_path)\n",
    "\n",
    "# Write the data in Delta format\n",
    "df_parquet.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Register the Delta table\n",
    "spark.sql(f\"CREATE TABLE delta_table USING DELTA LOCATION '{delta_path}'\")\n",
    "\n",
    "# Query the newly converted Delta table\n",
    "df_converted = spark.sql(\"SELECT * FROM delta_table\")\n",
    "df_converted.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2164644895540875,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "task-1,2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
